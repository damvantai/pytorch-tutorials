{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import argparse\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.select = ['0', '5', '10', '19', '28']\n",
    "        self.vgg = models.vgg19(pretrained=True).features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for name, layer in self.vgg._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in self.select:\n",
    "                features.append(x)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = models.vgg19(pretrained=True).features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modules = (models.vgg19(pretrained=True).features._modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = models.vgg19(pretrained=True).features._modules.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = 'png/content.png'\n",
    "style = 'png/style.png'\n",
    "max_size = 400\n",
    "total_step = 2000\n",
    "log_step = 10\n",
    "sample_step = 500\n",
    "style_weight = 100\n",
    "lr = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                        std=(0.229, 0.224, 0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "image = Image.open(content)\n",
    "\n",
    "scale = max_size / max(image.size)\n",
    "size = np.array(image.size) *scale\n",
    "image = image.resize(size.astype(int), Image.ANTIALIAS)\n",
    "\n",
    "# transform\n",
    "image = transform(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 2.0605,  2.0605,  2.0605,  ...,  2.1119,  2.1119,  2.0777],\n",
       "          [ 2.0605,  2.0605,  2.0605,  ...,  2.1119,  2.1119,  2.0948],\n",
       "          [ 2.0605,  2.0605,  2.0605,  ...,  2.1119,  2.1290,  2.0948],\n",
       "          ...,\n",
       "          [ 1.9578,  1.9578,  1.9578,  ...,  2.1290,  2.1119,  2.1462],\n",
       "          [ 1.9578,  1.9578,  1.9578,  ...,  2.1290,  2.1290,  2.1633],\n",
       "          [ 1.9578,  1.9578,  1.9578,  ...,  2.1119,  2.1633,  2.1804]],\n",
       "\n",
       "         [[ 2.0959,  2.0959,  2.0784,  ...,  2.0959,  2.0959,  2.1134],\n",
       "          [ 2.1134,  2.0784,  2.0784,  ...,  2.0959,  2.0959,  2.1134],\n",
       "          [ 2.0959,  2.0959,  2.0959,  ...,  2.0784,  2.0784,  2.1134],\n",
       "          ...,\n",
       "          [ 1.8683,  1.8683,  1.8683,  ...,  2.2185,  2.2010,  2.1835],\n",
       "          [ 1.8683,  1.8683,  1.8683,  ...,  2.2010,  2.2010,  2.2010],\n",
       "          [ 1.8683,  1.8683,  1.8683,  ...,  2.1835,  2.1835,  2.2010]],\n",
       "\n",
       "         [[ 2.0823,  2.0997,  2.0823,  ...,  2.0997,  2.0997,  2.0997],\n",
       "          [ 2.0823,  2.0823,  2.0823,  ...,  2.0997,  2.0997,  2.0997],\n",
       "          [ 2.0823,  2.0648,  2.0823,  ...,  2.0997,  2.0997,  2.0997],\n",
       "          ...,\n",
       "          [ 1.7511,  1.7511,  1.7511,  ...,  2.2566,  2.2391,  2.2043],\n",
       "          [ 1.7511,  1.7511,  1.7511,  ...,  2.2566,  2.2391,  2.2566],\n",
       "          [ 1.7511,  1.7511,  1.7337,  ...,  2.2217,  2.2043,  2.2391]]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.4500, -1.2617, -1.3130,  ..., -1.3644, -1.3473, -1.3130],\n",
       "          [-1.3130, -1.1589, -1.1760,  ..., -1.1760, -1.2274, -1.2617],\n",
       "          [-1.2274, -1.0048, -0.9877,  ..., -0.9877, -1.0219, -1.0048],\n",
       "          ...,\n",
       "          [-0.0629,  1.4440,  0.5364,  ...,  1.9407,  1.7865,  1.7180],\n",
       "          [-0.5082,  1.3755,  2.1804,  ...,  1.9235,  1.7352,  1.7009],\n",
       "          [-0.3541,  1.0502,  0.8447,  ...,  1.9064,  1.7180,  1.7009]],\n",
       "\n",
       "         [[-1.3704, -1.4055, -1.3880,  ..., -1.3880, -1.3354, -1.3004],\n",
       "          [-1.3354, -1.4405, -1.4405,  ..., -1.4055, -1.4230, -1.4230],\n",
       "          [-1.4580, -1.5280, -1.5630,  ..., -1.5630, -1.5280, -1.4580],\n",
       "          ...,\n",
       "          [-0.4076,  1.1331,  0.1877,  ...,  1.1506,  1.1681,  1.1856],\n",
       "          [-0.8102,  1.1506,  2.0434,  ...,  1.1331,  1.1331,  1.2031],\n",
       "          [-0.6176,  0.8004,  0.6254,  ...,  1.0980,  1.0980,  1.1856]],\n",
       "\n",
       "         [[-1.0724, -1.1073, -1.1596,  ..., -1.1596, -1.1596, -1.1421],\n",
       "          [-1.0724, -1.1596, -1.2119,  ..., -1.2119, -1.2467, -1.2816],\n",
       "          [-1.2293, -1.2641, -1.3513,  ..., -1.4384, -1.4036, -1.3513],\n",
       "          ...,\n",
       "          [-0.2707,  1.2282,  0.2522,  ..., -1.2990, -0.9156, -0.4450],\n",
       "          [-0.6715,  1.2631,  2.1171,  ..., -0.9678, -0.7587, -0.3753],\n",
       "          [-0.4798,  0.9319,  0.7054,  ..., -0.6367, -0.5844, -0.3404]]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_image = Image.open(style)\n",
    "\n",
    "style_image = style_image.resize([image.size(2), image.size(3)], Image.LANCZOS)\n",
    "style_image = transform(style_image).unsqueeze(0)\n",
    "style_image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = image.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([target], lr, betas=[0.5, 0.999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGGNet().to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(2000):\n",
    "    target_features = vgg(target)\n",
    "    content_features = vgg(image)\n",
    "    style_features = vgg(style)\n",
    "    \n",
    "    style_loss = 0\n",
    "    content_loss = 0\n",
    "    for f1, f2, f3 in zip(target_features, content_features, style_features):\n",
    "        content_loss += torch.mean((f1-f2)**2)\n",
    "        \n",
    "        _, c, h, w = f1.size()\n",
    "        f1 = f1.view(c, h * w)\n",
    "        f3 = f3.view(c, h * w)\n",
    "        \n",
    "        f1 = torch.mm(f1, f1.t())\n",
    "        f3 = torch.mm(f3, f3.t())\n",
    "        \n",
    "        style_loss += torch.mean((f1 - f3)**2) / (c*h*w)\n",
    "    \n",
    "    # Compute total loss, backprop and optimize\n",
    "    loss = content_loss + 100 * style_loss\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (step + 1) % 10 == 0:\n",
    "        print('Step [{}/{}], Content Loss: {:.4f}, Style Loss: {:.4f}'.format(step+1, 2000, content_loss.item(), style_loss.item()))\n",
    "        \n",
    "    if (step + 1) % 500 == 0:\n",
    "        denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n",
    "        img = target.clone().squeeze()\n",
    "        img = denorm(img).clamp_(0, 1)\n",
    "        torchvision.utils.save_image(img, 'output-{}.png'.format(step+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
